{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinaal Leren - Sprint 3: Multiple data sources\n",
    "\n",
    "## Task:\n",
    "Use the images or external data sources (eg. Ghent open data)\n",
    "\n",
    "Possible tasks:\n",
    "* Use the images to find similar listings (e.g. similar interior style)\n",
    "* Cluster the images\n",
    "* Automatically detect attributes of the listing (e.g. garden, bath, shower, washing machine, ...) based on the images\n",
    "* Find duplicate listings based on the images\n",
    "* Detect anomalies (rooms that look very different)\n",
    "* Predict which room a picture is taken in (bedroom, bathroom, outside, ...)\n",
    "* Combine with external data to better predict the price (e.g. location of public transport, proximity to attractions, ...)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work table\n",
    "\n",
    "|Task|Peter Bonnarens|Philip Kukoba|Lennert Franssens|\n",
    "|------|------|------|------|\n",
    "|1. Loading and cleaning dataset  | X |  |  |\n",
    "|2. Find similar listings  | X |  |   |\n",
    "|3. Find duplicate listings  | X |  |   |\n",
    "|4. Cluster the images  |   |   |  |\n",
    "|5. Detect attributes of the listing  |   |   |  |\n",
    "|6. Detect anomalies  |   |   |  |\n",
    "|7. Predict which room a picture is taken in  |   |   |  |\n",
    "|8. Use external data to better predict the price  |   |   |  |\n",
    "|9. Extra 1  |  X |   |  |\n",
    "|10. Extra 2  |   |   |  |\n",
    "|11. Extra 3  |   |   |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1 - LOADING AND CLEANING THE DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and read dataset\n",
    "Here we import the needed packages for this project and read the 'listings' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from scipy.spatial import distance\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Model\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# remove warnings from output\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# matplotplib figure size in inches\n",
    "plt.rcParams['figure.figsize'] = 15,12\n",
    "\n",
    "# reading the dataset and making a pandas dataframe\n",
    "listings = pd.read_csv(\"data/listings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the 'listings' dataset\n",
    "\n",
    "TODO: write text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean the listings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/3 - FINDING SIMILAR AND DUPLICATE LISTINGS BASED ON THE IMAGES**\n",
    "\n",
    "To find listings that are similar, we use what is called **content-based image retrieval** (**CBIR**). This is the same principle that is used in search engines, a certain image is given and the engine will retrieve similar images. For this task, features will need to be extracted from all images so that we can compare them. Instead of writing our own model, we use a smart approach that is described in the following notebook: https://github.com/ml4a/ml4a/blob/master/examples/info_retrieval/image-search.ipynb.\n",
    "\n",
    "First, we will load a pre-trained neural network from Keras: `VGG16`. This is a convolutional neural network model that is specifically made for image classification. It contains 16 layers, has an accuracy rate of 92.7% and was trained for weeks. The architecture is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16 architecture](vgg16_architecture.jpeg \"VGG16 architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the model, we will use the pre-trained weights from `ImageNet` and include the 3 fully-connected layers at the top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a highly effective image classifier trained on the `ImageNet` database. The problem is that we are not interested in classifying our images, but rather comparing them based on features to detect similarities. We expect that this classifier must form a very effective representation of an image in order to be able to classify it with such high accuracy. We can use this to our advantage by dropping the last dense layer (the `predictions` layer) where the actual classification is happening, and keep the rest of the network. This way, we actually have a highly efficient feature extractor (`feat_extractor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "feat_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to to test our feature extractor, we first define a helper function to load and pre-process an image from a specified path. Our `VGG16` model expects an input vector of the correct dimensions, namely a color image of size 224x224. To do the pre-processing, `Keras` has a built-in function called `preprocess_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load a chosen image, perform the prediction of our `feat_extractor` and plot the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, x = load_image(\"./data/images/187870/187870_0.jpg\")\n",
    "feat = feat_extractor.predict(x)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(feat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function return an array with one element per image. Each element containes a 4096-element array, which is the activations of the last fully-connected layer `fc2`. Our expectation is that these activations form a good representation of the image, such that similar images would produce similar activations.\n",
    "\n",
    "Next we walk through our image directory and only keep the paths to the images that have a `.jpg` extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"./data/images\"\n",
    "image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_path) for f in filenames if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "print(\"keeping %d images to analyze\" % len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we start a loop that will open each image, extract its feature vector, and append it to a list called `features` which will contain our activations for each image. Because this process takes a long time to finish, we only ran this code once and saved the features as a numpy array into the file `features.npy`. When we want to use these features in our program, we just load the numpy array from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: takes a long time to run! \n",
    "\n",
    "# tic = time.process_time()\n",
    "\n",
    "# features = []\n",
    "# for i, image_path in enumerate(images):\n",
    "#     if i % 500 == 0:\n",
    "#         toc = time.process_time()\n",
    "#         elap = toc-tic\n",
    "#         print(\"analyzing image %d / %d. Time: %4.4f seconds.\" % (i, len(images),elap))\n",
    "#         tic = time.process_time()\n",
    "#     img, x = load_image(image_path)\n",
    "#     feat = feat_extractor.predict(x)[0]\n",
    "#     features.append(feat)\n",
    "\n",
    "# print('finished extracting features for %d images' % len(images))\n",
    "# np.save('features.npy', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove redundancy in the 4096-bit feature vectors and reduce the dimensionality, we use **principal component analysis (PCA)** (link: https://en.wikipedia.org/wiki/Principal_component_analysis). This will not only remove possible skew that would occur in our similarity comparisons towards over-represented features, but also speed up our computation time since our feature vectors will have a much smaller length. Here we chose to keep 300 principal components, but this number can be tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(\"features.npy\")\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(features)\n",
    "pca_features = pca.transform(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define 2 more helper functions: \n",
    "\n",
    "`get_closest_images` which calculates the cosine distance between the pca_features of our query image and the pca_features of every other image we have, and then orders them so we keep to `num_results` images with the closest cosine distance (or thus the most similar images).\n",
    "\n",
    "`get_concatenated_images` which helps us load those similar images and append them to `concat_image` so we can display them in a row using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_images(query_image_idx, num_results=5):\n",
    "    distances = [ distance.cosine(pca_features[query_image_idx], feat) for feat in pca_features ]\n",
    "    idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:num_results+1]\n",
    "    return idx_closest\n",
    "\n",
    "def get_concatenated_images(indexes, thumb_height):\n",
    "    thumbs = []\n",
    "    for idx in indexes:\n",
    "        img = image.load_img(images[idx])\n",
    "        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))\n",
    "        thumbs.append(img)\n",
    "    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return concat_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we try out our model by choosing a random query image, getting the most similar images to it and displaying all of them so we can visually compare if our model does its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a query on a random image\n",
    "query_image_idx = 125\n",
    "idx_closest = get_closest_images(query_image_idx)\n",
    "query_image = get_concatenated_images([query_image_idx], 300)\n",
    "results_image = get_concatenated_images(idx_closest, 200)\n",
    "\n",
    "# display the query image\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(query_image)\n",
    "plt.title(\"query image (%d)\" % query_image_idx)\n",
    "\n",
    "# display the resulting images\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.imshow(results_image)\n",
    "plt.title(\"result images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is essentially a working reverse image search engine: we give it an image, and the model returns us a couple other images that are similar (or identical) to it. However, we still need to solve the problem we had in the beginning: searching similar and duplicate **listings**. In order to do this, we can ofcourse use the features previously created by our model.\n",
    "\n",
    "We will try the following approach: we will create a dictionary that has the `listing_id` as a key, and a list as the value. We will loop over all our images and use our reverse image search model to get the most similar images to it. Then, we will add the `listing_id` of our query image as a key in our dictionary (if it doesn't exist already) and append the list with the `listing_id`'s of our similar images. Then we can use the `Counter` class to get the listings that are mentioned the most for our given `listing_id`.\n",
    "\n",
    "This approach will essentially tell us that x images of a listing have a strong similarity with x images of another listing. The higher the amount of similar images, the higher the chance that these 2 listings are (near-) duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# listing_dict = {}\n",
    "# length = range(len(images))\n",
    "# for idx in length:\n",
    "#     listing_id = images[idx].split(\"_\")[0].split(\"\\\\\")[-1]\n",
    "\n",
    "#     if listing_id not in listing_dict:\n",
    "#         listing_dict[listing_id] = []\n",
    "\n",
    "#     idx_closest = get_closest_images(idx, 10)\n",
    "#     for idx_close in idx_closest:\n",
    "#         dup_listing_id = images[idx_close].split(\"_\")[0].split(\"\\\\\")[-1]\n",
    "#         if listing_id != dup_listing_id:\n",
    "#             listing_dict[listing_id].append(dup_listing_id)\n",
    "    \n",
    "# for id in listing_dict:\n",
    "#     listing_dict[id] = Counter(listing_dict[id]).most_common(5)\n",
    "\n",
    "# with open('listing_sim_dict.pkl', 'wb+') as f:\n",
    "#     pickle.dump(listing_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listing_sim_dict.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "\n",
    "sorted(loaded_dict.items(), key=lambda e: e[1][0][1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_images(list_id):\n",
    "    thumbs = []\n",
    "    path = \"./data/images/\" + list_id\n",
    "    \n",
    "    for i in os.listdir(path):\n",
    "        img = image.load_img(path + \"/\" + i)\n",
    "        img = img.resize((int(img.width * 500 / img.height), 500))\n",
    "        thumbs.append(img)\n",
    "    res_img = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return thumbs\n",
    "\n",
    "res_img1 = load_and_concat_images(\"24400600\")\n",
    "res_img2 = load_and_concat_images(\"27719749\")\n",
    "res_img3 = load_and_concat_images(\"22094809\")\n",
    "\n",
    "# display the resulting images\n",
    "n_row = 4\n",
    "n_col = 6\n",
    "\n",
    "fig, axs = plt.subplots(n_row, n_col, figsize=(20, 16))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(res_img1, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "axs[22].axis('off')\n",
    "axs[23].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(n_row, n_col, figsize=(20, 16))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(res_img2, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "axs[22].axis('off')\n",
    "axs[23].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(n_row, n_col, figsize=(20, 16))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(res_img3, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "axs[21].axis('off')\n",
    "axs[22].axis('off')\n",
    "axs[23].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4 - CLUSTERING THE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 - AUTOMATICALLY DETECT ATTRIBUTES OF LISTINGS BASED ON THE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6 - DETECTING ANOMALIES BASED ON THE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7 - PREDICTING WHICH ROOM A PICTURE IS TAKEN IN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8 - USING EXTERNAL DATA TO BETTER PREDICT THE PRICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9 - EXTRA 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link: https://medium.com/nerd-for-tech/image-classification-using-transfer-learning-vgg-16-2dc2221be34c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. image classifier - wheelchair accessible or not?\n",
    "2. staircase detector?\n",
    "3. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10 - EXTRA 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **11 - EXTRA 3**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ece5b934e5507bb10d71fe477ed4928a6f8f0a8a03212c3c5909a754786da47a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
